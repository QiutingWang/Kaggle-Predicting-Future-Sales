{"cells":[{"cell_type":"markdown","metadata":{},"source":["# **Predictive Future Sales**\n","**1. Exploratory Data Analysis**"]},{"cell_type":"markdown","metadata":{},"source":["**Introduction:**\n","In this competition you are provided with a challenging time-series dataset consisting of daily sales data, kindly provided by one of the largest Russian software firms - 1C Company. We are asking you to predict total sales for every product and store in the next month. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products changes every month. \n","This notebook contains general information about the methods using which you can approach the problem statement.\n","\n","Lets first discuss what we are given and what we have to predict. About our dataset:\n","\n","The features in our *training* data:\n","\n","1. date - every date of items sold\n","2. date_block_num - this number is given to every month\n","3. shop_id - unique number of every shop\n","4. item_id - unique number of every item\n","5. item_price - price of every item\n","6. item_cnt_day - number of items sold on a particular day\n","\n","The features in our *testing* data :\n","\n","1. ID - unique for every (shop_id,item_id) pair.\n","2. shop_id - unique number of every shop\n","3. item_id - unique number of every item\n","\n","Daily historical sales are given *from Jan 2013 to Oct 2015*. The **task** is to predict total sales for every product and store in the next month. The **goal** is here to minimise the performance metric:*RMSE score*."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:19.890663Z","iopub.status.busy":"2022-07-10T12:32:19.890081Z","iopub.status.idle":"2022-07-10T12:32:20.387241Z","shell.execute_reply":"2022-07-10T12:32:20.386129Z","shell.execute_reply.started":"2022-07-10T12:32:19.890620Z"},"trusted":true},"outputs":[],"source":["#import python packages and libs#\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import datetime\n","import warnings\n","from xgboost import XGBRegressor\n","from xgboost import plot_importance\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:20.389796Z","iopub.status.busy":"2022-07-10T12:32:20.389320Z","iopub.status.idle":"2022-07-10T12:32:23.258631Z","shell.execute_reply":"2022-07-10T12:32:23.257622Z","shell.execute_reply.started":"2022-07-10T12:32:20.389748Z"},"trusted":true},"outputs":[],"source":["#import the raw datasets#\n","train=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\n","items=pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\n","categories=pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n","shops=pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n","test=pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["**Dataset Overview**\n","\n","Datasets apart from the test and train dataset that are given to us:\n","\n","1. item_categories.csv - the item category name along with the category ID\n","2. items.csv - the item name along with item ID and category ID\n","3. shops.csv - the shop name along with shop ID"]},{"cell_type":"markdown","metadata":{},"source":["**1.1Categories**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.260147Z","iopub.status.busy":"2022-07-10T12:32:23.259805Z","iopub.status.idle":"2022-07-10T12:32:23.318861Z","shell.execute_reply":"2022-07-10T12:32:23.317806Z","shell.execute_reply.started":"2022-07-10T12:32:23.260116Z"},"trusted":true},"outputs":[],"source":["print(\"First 5 Entries\")\n","print(categories.head(5))\n","\n","print(\"Information\")\n","print(categories.info())\n","\n","print(\"Data Types\")\n","print(categories.dtypes)\n","\n","print(\"Missing Value\")\n","print(categories.isnull().sum())\n","\n","print(\"Null Value\")\n","print(categories.isna().sum())\n","\n","print(\"Shape\")\n","print(categories.shape)\n","\n","print('Description')\n","categories.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**1.2 Items**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.322276Z","iopub.status.busy":"2022-07-10T12:32:23.321360Z","iopub.status.idle":"2022-07-10T12:32:23.367945Z","shell.execute_reply":"2022-07-10T12:32:23.366829Z","shell.execute_reply.started":"2022-07-10T12:32:23.322201Z"},"trusted":true},"outputs":[],"source":["print(\"First 5 Entries\")\n","print(items.head(5))\n","\n","print(\"Information\")\n","print(items.info())\n","\n","print(\"Data Types\")\n","print(items.dtypes)\n","\n","print(\"Missing value\")\n","print(items.isnull().sum())\n","\n","print(\"Null value\")\n","print(items.isna().sum())\n","\n","print(\"Shape of Data\")\n","print(items.shape)\n","\n","print('Description')\n","items.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**1.3 Shops**"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.369697Z","iopub.status.busy":"2022-07-10T12:32:23.369273Z","iopub.status.idle":"2022-07-10T12:32:23.400730Z","shell.execute_reply":"2022-07-10T12:32:23.399673Z","shell.execute_reply.started":"2022-07-10T12:32:23.369664Z"},"trusted":true},"outputs":[],"source":["print(\"First 5 Entries\")\n","print(shops.head(5))\n","\n","print(\"Info\")\n","print(shops.info())\n","\n","print(\"Data Types\")\n","print(shops.dtypes)\n","\n","print(\"Missing value\")\n","print(shops.isnull().sum())\n","\n","print(\"Null value\")\n","print(shops.isna().sum())\n","\n","print(\"Shape of Data\")\n","print(shops.shape)\n","\n","print('Description')\n","shops.describe()"]},{"cell_type":"markdown","metadata":{},"source":["**1.4 Training**"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.402882Z","iopub.status.busy":"2022-07-10T12:32:23.402400Z","iopub.status.idle":"2022-07-10T12:32:23.413447Z","shell.execute_reply":"2022-07-10T12:32:23.412307Z","shell.execute_reply.started":"2022-07-10T12:32:23.402836Z"},"trusted":true},"outputs":[],"source":["print(\"First 5 entries\")\n","print(train.head())"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.415797Z","iopub.status.busy":"2022-07-10T12:32:23.415156Z","iopub.status.idle":"2022-07-10T12:32:23.425995Z","shell.execute_reply":"2022-07-10T12:32:23.424964Z","shell.execute_reply.started":"2022-07-10T12:32:23.415753Z"},"trusted":true},"outputs":[],"source":["print(\"Shape\")\n","print(train.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.427700Z","iopub.status.busy":"2022-07-10T12:32:23.427371Z","iopub.status.idle":"2022-07-10T12:32:23.441783Z","shell.execute_reply":"2022-07-10T12:32:23.440581Z","shell.execute_reply.started":"2022-07-10T12:32:23.427663Z"},"trusted":true},"outputs":[],"source":["print(\"Infomation\")\n","print(train.info())"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.443603Z","iopub.status.busy":"2022-07-10T12:32:23.443185Z","iopub.status.idle":"2022-07-10T12:32:23.450554Z","shell.execute_reply":"2022-07-10T12:32:23.449306Z","shell.execute_reply.started":"2022-07-10T12:32:23.443566Z"},"trusted":true},"outputs":[],"source":["print(\"Data Types\")\n","print(train.dtypes)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.455084Z","iopub.status.busy":"2022-07-10T12:32:23.454541Z","iopub.status.idle":"2022-07-10T12:32:23.801468Z","shell.execute_reply":"2022-07-10T12:32:23.800296Z","shell.execute_reply.started":"2022-07-10T12:32:23.455045Z"},"trusted":true},"outputs":[],"source":["print(\"Missing NaN values\")\n","print(train.isnull().sum())"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:23.803327Z","iopub.status.busy":"2022-07-10T12:32:23.802845Z","iopub.status.idle":"2022-07-10T12:32:24.142020Z","shell.execute_reply":"2022-07-10T12:32:24.141030Z","shell.execute_reply.started":"2022-07-10T12:32:23.803290Z"},"trusted":true},"outputs":[],"source":["print(\"Null Values\")\n","print(train.isna().sum())"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:24.143531Z","iopub.status.busy":"2022-07-10T12:32:24.143199Z","iopub.status.idle":"2022-07-10T12:32:24.631785Z","shell.execute_reply":"2022-07-10T12:32:24.630756Z","shell.execute_reply.started":"2022-07-10T12:32:24.143503Z"},"trusted":true},"outputs":[],"source":["train.describe()"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:24.633289Z","iopub.status.busy":"2022-07-10T12:32:24.632969Z","iopub.status.idle":"2022-07-10T12:32:25.184712Z","shell.execute_reply":"2022-07-10T12:32:25.183984Z","shell.execute_reply.started":"2022-07-10T12:32:24.633261Z"},"trusted":true},"outputs":[],"source":["#Change the datetime format\n","train['date'] = pd.to_datetime(train['date'], format = '%d.%m.%Y')\n","train"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:25.186659Z","iopub.status.busy":"2022-07-10T12:32:25.185871Z","iopub.status.idle":"2022-07-10T12:32:26.309977Z","shell.execute_reply":"2022-07-10T12:32:26.309054Z","shell.execute_reply.started":"2022-07-10T12:32:25.186619Z"},"trusted":true},"outputs":[],"source":["#join the training dataset\n","train = train.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(categories, on='item_category_id', rsuffix='_')"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:26.312482Z","iopub.status.busy":"2022-07-10T12:32:26.311381Z","iopub.status.idle":"2022-07-10T12:32:26.344542Z","shell.execute_reply":"2022-07-10T12:32:26.343315Z","shell.execute_reply.started":"2022-07-10T12:32:26.312429Z"},"trusted":true},"outputs":[],"source":["print('Min date from train set: %s' % train['date'].min().date())\n","print('Max date from train set: %s' % train['date'].max().date())"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:26.346018Z","iopub.status.busy":"2022-07-10T12:32:26.345700Z","iopub.status.idle":"2022-07-10T12:32:29.771703Z","shell.execute_reply":"2022-07-10T12:32:29.770612Z","shell.execute_reply.started":"2022-07-10T12:32:26.345982Z"},"trusted":true},"outputs":[],"source":["#Note that this function can be modified by choosing to calculate duplicates over only a subset of features, keeping the last entry, etc.\n","train.drop_duplicates(inplace=True,keep='first')\n","print(train)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:29.773666Z","iopub.status.busy":"2022-07-10T12:32:29.773232Z","iopub.status.idle":"2022-07-10T12:32:30.025438Z","shell.execute_reply":"2022-07-10T12:32:30.024429Z","shell.execute_reply.started":"2022-07-10T12:32:29.773622Z"},"trusted":true},"outputs":[],"source":["#Data cleaning and remove rows with negative item price.\n","train = train.query('item_price > 0') \n","train"]},{"cell_type":"markdown","metadata":{},"source":["**Data Leakage🌟**\n","\n","When there's data leakage in the data used for machine learning model,we will get a high train and test accuracy, implying that the model is good enough for production. It will neither underfit or overfit.\n","\n","However, when implementing the machine learning model in production, it will no longer be introduced to one feature. Because it is not available when you need the model’s predictions. The feature missing might even be the most important feature for determining the right class: the leaked data.\n","\n","When implying the machine learning model in production, we will see that the predictions are not reliable.\n","\n","We'll only be using only the \"shop_id\" and \"item_id\" that appear on the test dataset. The idea here is that we know on which shops and items we are going to predict, because of the test set, training on those rows you will get a data distribution closer to the test set, so probably this is a better idea."]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:30.027656Z","iopub.status.busy":"2022-07-10T12:32:30.026886Z","iopub.status.idle":"2022-07-10T12:32:30.397336Z","shell.execute_reply":"2022-07-10T12:32:30.396336Z","shell.execute_reply.started":"2022-07-10T12:32:30.027608Z"},"trusted":true},"outputs":[],"source":["test_shop_ids = test['shop_id'].unique()\n","test_item_ids = test['item_id'].unique()\n","# Only shops that exist in test set.\n","lk_train = train[train['shop_id'].isin(test_shop_ids)]\n","# Only items that exist in test set.\n","lk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]\n","\n","print('Data set size before leaking:', train.shape[0])\n","print('Data set size after leaking:', lk_train.shape[0])"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:30.399308Z","iopub.status.busy":"2022-07-10T12:32:30.398934Z","iopub.status.idle":"2022-07-10T12:32:30.425765Z","shell.execute_reply":"2022-07-10T12:32:30.424978Z","shell.execute_reply.started":"2022-07-10T12:32:30.399275Z"},"trusted":true},"outputs":[],"source":["#After leakage,drop the text columns since they are not significant for predictions.\n","lk_train"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:30.427381Z","iopub.status.busy":"2022-07-10T12:32:30.426991Z","iopub.status.idle":"2022-07-10T12:32:30.468446Z","shell.execute_reply":"2022-07-10T12:32:30.467518Z","shell.execute_reply.started":"2022-07-10T12:32:30.427350Z"},"trusted":true},"outputs":[],"source":["lk_train.drop(['item_name','shop_name','item_category_name'],axis=1)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:30.470089Z","iopub.status.busy":"2022-07-10T12:32:30.469767Z","iopub.status.idle":"2022-07-10T12:32:30.511404Z","shell.execute_reply":"2022-07-10T12:32:30.510623Z","shell.execute_reply.started":"2022-07-10T12:32:30.470059Z"},"trusted":true},"outputs":[],"source":["train_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day','item_category_id']]\n","train_monthly.head()"]},{"cell_type":"markdown","metadata":{},"source":["**1.5 EDA for Item_id**\n","1. How the monthly sum and mean vary for with item_id for each month\n","2. The outdated items, over perhaps the last 6 months."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:30.512850Z","iopub.status.busy":"2022-07-10T12:32:30.512549Z","iopub.status.idle":"2022-07-10T12:32:31.266506Z","shell.execute_reply":"2022-07-10T12:32:31.265326Z","shell.execute_reply.started":"2022-07-10T12:32:30.512823Z"},"trusted":true},"outputs":[],"source":["train_by_item_id = train.pivot_table(index=['item_id'],values=['item_cnt_day'], columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n","train_by_item_id.columns = train_by_item_id.columns.droplevel().map(str)\n","train_by_item_id = train_by_item_id.reset_index(drop=True).rename_axis(None, axis=1)\n","train_by_item_id.columns.values[0] = 'item_id'\n","# print(train_by_item_id.sum()[1:])\n","train_by_item_id.sum()[1:].plot(legend=True, label=\"Monthly sum\")"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.269385Z","iopub.status.busy":"2022-07-10T12:32:31.267952Z","iopub.status.idle":"2022-07-10T12:32:31.463837Z","shell.execute_reply":"2022-07-10T12:32:31.463105Z","shell.execute_reply.started":"2022-07-10T12:32:31.269348Z"},"trusted":true},"outputs":[],"source":["train_by_item_id.mean()[1:].plot(legend=True, label=\"Monthly mean\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.465518Z","iopub.status.busy":"2022-07-10T12:32:31.464764Z","iopub.status.idle":"2022-07-10T12:32:31.493516Z","shell.execute_reply":"2022-07-10T12:32:31.492504Z","shell.execute_reply.started":"2022-07-10T12:32:31.465486Z"},"trusted":true},"outputs":[],"source":["train.item_id.nunique()"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.495646Z","iopub.status.busy":"2022-07-10T12:32:31.494965Z","iopub.status.idle":"2022-07-10T12:32:31.505731Z","shell.execute_reply":"2022-07-10T12:32:31.504922Z","shell.execute_reply.started":"2022-07-10T12:32:31.495608Z"},"trusted":true},"outputs":[],"source":["#outdated after month 27 in our 34 month window\n","outdated_items = train_by_item_id[train_by_item_id.loc[:,'27':].sum(axis=1)==0] \n","print('Outdated items:', len(outdated_items))"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.507932Z","iopub.status.busy":"2022-07-10T12:32:31.507089Z","iopub.status.idle":"2022-07-10T12:32:31.520997Z","shell.execute_reply":"2022-07-10T12:32:31.520009Z","shell.execute_reply.started":"2022-07-10T12:32:31.507846Z"},"trusted":true},"outputs":[],"source":["print('Outdated items in test set:', len(test[test['item_id'].isin(outdated_items['item_id'])]))"]},{"cell_type":"markdown","metadata":{},"source":["**Summary**\n","1. There are no missing values at all. \n","2. Number of sold items usually declines over the year.\n","3. There are peaks in December and similar item count zig-zag behavior can be seen in June-July-August. This can be due to these periods being vacation time or possibly a national holiday.\n","4. 12391 of 21807 items in the train set are outdated since the last 6 months, which is quite huge.\n","5. For test set: 6888 outdated."]},{"cell_type":"markdown","metadata":{},"source":["**1.6 EDA for Shop_id**"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.522950Z","iopub.status.busy":"2022-07-10T12:32:31.522350Z","iopub.status.idle":"2022-07-10T12:32:31.821259Z","shell.execute_reply":"2022-07-10T12:32:31.820305Z","shell.execute_reply.started":"2022-07-10T12:32:31.522915Z"},"trusted":true},"outputs":[],"source":["train_by_shop_id = train.pivot_table(index=['shop_id'],values=['item_cnt_day'], columns='date_block_num', aggfunc=np.sum, fill_value=0).reset_index()\n","train_by_shop_id.columns = train_by_shop_id.columns.droplevel().map(str)\n","train_by_shop_id = train_by_shop_id.reset_index(drop=True).rename_axis(None, axis=1)\n","train_by_shop_id.columns.values[0] = 'shop_id'\n","\n","for i in range(6,34):\n","    print('Does not exist in month',i,train_by_shop_id['shop_id'][train_by_shop_id.loc[:,'0':str(i)].sum(axis=1)==0].unique())\n","\n","for i in range(6,28):\n","    print('Shop is outdated for month',i,train_by_shop_id['shop_id'][train_by_shop_id.loc[:,str(i):].sum(axis=1)==0].unique())"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.827376Z","iopub.status.busy":"2022-07-10T12:32:31.826978Z","iopub.status.idle":"2022-07-10T12:32:31.834787Z","shell.execute_reply":"2022-07-10T12:32:31.833424Z","shell.execute_reply.started":"2022-07-10T12:32:31.827336Z"},"trusted":true},"outputs":[],"source":["print('Recently opened shop items:', len(test[test['shop_id']==36])) "]},{"cell_type":"markdown","metadata":{},"source":["**2.Data Preprocessing**\n","\n","1. We drop irrelevant or less important features\n","2. We need to process datetime, from daily-->monthly"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.836770Z","iopub.status.busy":"2022-07-10T12:32:31.836181Z","iopub.status.idle":"2022-07-10T12:32:31.880628Z","shell.execute_reply":"2022-07-10T12:32:31.879696Z","shell.execute_reply.started":"2022-07-10T12:32:31.836729Z"},"trusted":true},"outputs":[],"source":["#Select useful features\n","train_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]\n","train_monthly.head()"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:31.882333Z","iopub.status.busy":"2022-07-10T12:32:31.881955Z","iopub.status.idle":"2022-07-10T12:32:32.596152Z","shell.execute_reply":"2022-07-10T12:32:32.595345Z","shell.execute_reply.started":"2022-07-10T12:32:31.882300Z"},"trusted":true},"outputs":[],"source":["# Group by month in this case \"date_block_num\" and aggregate features.\n","train_monthly = train_monthly.sort_values(['date']).groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\n","train_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:32.597995Z","iopub.status.busy":"2022-07-10T12:32:32.597208Z","iopub.status.idle":"2022-07-10T12:32:32.603300Z","shell.execute_reply":"2022-07-10T12:32:32.602299Z","shell.execute_reply.started":"2022-07-10T12:32:32.597959Z"},"trusted":true},"outputs":[],"source":["# Rename features.\n","train_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt','transactions']"]},{"cell_type":"markdown","metadata":{},"source":["We're using empty_df, we need a dataframe that will have combinations of months, shop_id, and item_id.\n","First create an empty dataframe, then iterate over the existing records to fill it, and finally fill the missing values with 0.\n","We're taking all possible combinations here since we have to tell the model that for those months the item count for a particular shop ID/item ID was zero instead of having any missing records."]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:32.605314Z","iopub.status.busy":"2022-07-10T12:32:32.604818Z","iopub.status.idle":"2022-07-10T12:32:52.381529Z","shell.execute_reply":"2022-07-10T12:32:52.380490Z","shell.execute_reply.started":"2022-07-10T12:32:32.605242Z"},"trusted":true},"outputs":[],"source":["shop_ids = train_monthly['shop_id'].unique()\n","item_ids = train_monthly['item_id'].unique()\n","empty_df = []\n","for i in range(34):\n","    for shop in shop_ids:\n","        for item in item_ids:\n","            empty_df.append([i, shop, item])\n","empty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:52.383117Z","iopub.status.busy":"2022-07-10T12:32:52.382750Z","iopub.status.idle":"2022-07-10T12:32:55.297331Z","shell.execute_reply":"2022-07-10T12:32:55.296318Z","shell.execute_reply.started":"2022-07-10T12:32:52.383085Z"},"trusted":true},"outputs":[],"source":["#Merge the train set with the complete set (missing records will be filled with 0).\n","train_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\n","train_monthly.fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:55.299450Z","iopub.status.busy":"2022-07-10T12:32:55.298681Z","iopub.status.idle":"2022-07-10T12:32:55.318969Z","shell.execute_reply":"2022-07-10T12:32:55.318124Z","shell.execute_reply.started":"2022-07-10T12:32:55.299406Z"},"trusted":true},"outputs":[],"source":["train_monthly.head()"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:55.320816Z","iopub.status.busy":"2022-07-10T12:32:55.320107Z","iopub.status.idle":"2022-07-10T12:32:58.859415Z","shell.execute_reply":"2022-07-10T12:32:58.858552Z","shell.execute_reply.started":"2022-07-10T12:32:55.320777Z"},"trusted":true},"outputs":[],"source":["train_monthly.describe()"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:32:58.861464Z","iopub.status.busy":"2022-07-10T12:32:58.861099Z","iopub.status.idle":"2022-07-10T12:33:09.792142Z","shell.execute_reply":"2022-07-10T12:33:09.790732Z","shell.execute_reply.started":"2022-07-10T12:32:58.861432Z"},"trusted":true},"outputs":[],"source":["# Extract time based features, this will be essential when we group the data based on month/year.\n","train_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x//12) + 2013))\n","train_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:09.794949Z","iopub.status.busy":"2022-07-10T12:33:09.794147Z","iopub.status.idle":"2022-07-10T12:33:11.316755Z","shell.execute_reply":"2022-07-10T12:33:11.315834Z","shell.execute_reply.started":"2022-07-10T12:33:09.794903Z"},"trusted":true},"outputs":[],"source":["#date_block_num covers a window of 34 months and takes values from 0 to 33, so to extract some 'monthly' features\n","gp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\n","gp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\n","gp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\n","gp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\n","gp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\n","gp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:11.319605Z","iopub.status.busy":"2022-07-10T12:33:11.318381Z","iopub.status.idle":"2022-07-10T12:33:13.105619Z","shell.execute_reply":"2022-07-10T12:33:13.104614Z","shell.execute_reply.started":"2022-07-10T12:33:11.319553Z"},"trusted":true},"outputs":[],"source":["plt.style.use('seaborn')\n","train.copy().set_index('date').item_cnt_day.resample('M').mean().plot()"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:13.107915Z","iopub.status.busy":"2022-07-10T12:33:13.107112Z","iopub.status.idle":"2022-07-10T12:33:13.503925Z","shell.execute_reply":"2022-07-10T12:33:13.502742Z","shell.execute_reply.started":"2022-07-10T12:33:13.107862Z"},"trusted":true},"outputs":[],"source":["f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\n","sns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_mean, ax=axes[0]).set_title(\"Monthly mean\")\n","sns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_sum, ax=axes[1]).set_title(\"Monthly sum\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Summary**\n","1. We have a trending increase of item sales count (mean) towards the ending of the year.\n","2. There are peaks in October, then dips in November, followed by an increase in December and similar item count zig-zag behavior can be seen in June-July-August. This can be due to: vacation time/national holidays."]},{"cell_type":"markdown","metadata":{},"source":["**Item Category Sales Viz**"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:13.506064Z","iopub.status.busy":"2022-07-10T12:33:13.505574Z","iopub.status.idle":"2022-07-10T12:33:14.813398Z","shell.execute_reply":"2022-07-10T12:33:14.812234Z","shell.execute_reply.started":"2022-07-10T12:33:13.506016Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\n","sns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly Mean\")\n","sns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly Sum\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In conclusion,few of the categories are dominating the market."]},{"cell_type":"markdown","metadata":{},"source":["**Shops Sales Viz**"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:14.815831Z","iopub.status.busy":"2022-07-10T12:33:14.814993Z","iopub.status.idle":"2022-07-10T12:33:15.695930Z","shell.execute_reply":"2022-07-10T12:33:15.694970Z","shell.execute_reply.started":"2022-07-10T12:33:14.815781Z"},"trusted":true},"outputs":[],"source":["fig, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\n","sns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\n","sns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Conclusion:** Most of the shops have a similar sell rate, but 3 of them have a much higher rate, because of their shop size."]},{"cell_type":"markdown","metadata":{},"source":["**Outlier Identification**"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:15.698627Z","iopub.status.busy":"2022-07-10T12:33:15.697487Z","iopub.status.idle":"2022-07-10T12:33:17.337597Z","shell.execute_reply":"2022-07-10T12:33:17.336582Z","shell.execute_reply.started":"2022-07-10T12:33:15.698551Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,4))\n","plt.xlim(-100, 3000)\n","sns.boxplot(x=train['item_cnt_day'])\n","print('Item Sale outliers:',train['item_id'][train['item_cnt_day']>500].unique())\n","\n","plt.figure(figsize=(15,4))\n","plt.xlim(train['item_price'].min(), train['item_price'].max())\n","sns.boxplot(x=train['item_price'])\n","print('Item price outliers:',train['item_id'][train['item_price']>50000].unique())"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:17.340112Z","iopub.status.busy":"2022-07-10T12:33:17.339291Z","iopub.status.idle":"2022-07-10T12:33:17.988995Z","shell.execute_reply":"2022-07-10T12:33:17.988012Z","shell.execute_reply.started":"2022-07-10T12:33:17.340054Z"},"trusted":true},"outputs":[],"source":["#drop the outliers\n","train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 500 and item_price < 50000')\n","train_monthly"]},{"cell_type":"markdown","metadata":{},"source":["**3.Feature Engineering**"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:17.990592Z","iopub.status.busy":"2022-07-10T12:33:17.990238Z","iopub.status.idle":"2022-07-10T12:33:21.111021Z","shell.execute_reply":"2022-07-10T12:33:21.110004Z","shell.execute_reply.started":"2022-07-10T12:33:17.990553Z"},"trusted":true},"outputs":[],"source":["#shift the time since it is a forecast problem\n","train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1) \n","train_monthly"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:21.113511Z","iopub.status.busy":"2022-07-10T12:33:21.112502Z","iopub.status.idle":"2022-07-10T12:33:21.380108Z","shell.execute_reply":"2022-07-10T12:33:21.379254Z","shell.execute_reply.started":"2022-07-10T12:33:21.113474Z"},"trusted":true},"outputs":[],"source":["#unify the price mark\n","train_monthly['item_price_unit'] = train_monthly['item_price'] // train_monthly['item_cnt']\n","train_monthly['item_price_unit'].fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:21.381912Z","iopub.status.busy":"2022-07-10T12:33:21.381380Z","iopub.status.idle":"2022-07-10T12:33:24.551565Z","shell.execute_reply":"2022-07-10T12:33:24.550393Z","shell.execute_reply.started":"2022-07-10T12:33:21.381880Z"},"trusted":true},"outputs":[],"source":["#grouping data with item_id\n","gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\n","gp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n","\n","train_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:24.553179Z","iopub.status.busy":"2022-07-10T12:33:24.552847Z","iopub.status.idle":"2022-07-10T12:33:24.669333Z","shell.execute_reply":"2022-07-10T12:33:24.668598Z","shell.execute_reply.started":"2022-07-10T12:33:24.553149Z"},"trusted":true},"outputs":[],"source":["train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\n","train_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']"]},{"cell_type":"markdown","metadata":{},"source":["**Rolling Window**\n","\n","1. Creating a rolling window with a specified size and perform calculations on the data\n","2. A time series problem that recent lag values are more predictive than older lag values\n","3. rolling() functions->perform rolling window functions.Regard window size as a parameter to group values."]},{"cell_type":"code","execution_count":50,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:33:24.672137Z","iopub.status.busy":"2022-07-10T12:33:24.671674Z","iopub.status.idle":"2022-07-10T12:39:51.640047Z","shell.execute_reply":"2022-07-10T12:39:51.638835Z","shell.execute_reply.started":"2022-07-10T12:33:24.672091Z"},"trusted":true},"outputs":[],"source":["#make rolling functions' parameters with lambda function\n","#Min value\n","f_min = lambda x: x.rolling(window=3, min_periods=1).min()\n","#Max value\n","f_max = lambda x: x.rolling(window=3, min_periods=1).max()\n","#Mean value\n","f_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n","#Standard deviation\n","f_std = lambda x: x.rolling(window=3, min_periods=1).std()\n","\n","function_list = [f_min, f_max, f_mean, f_std]\n","function_name = ['min', 'max', 'mean', 'std']\n","\n","for i in range(len(function_list)):\n","    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n","#Fill the empty std features with 0\n","train_monthly['item_cnt_std'].fillna(0, inplace=True)\n"]},{"cell_type":"markdown","metadata":{},"source":["**Lag Based Feature**\n","1. A lag features is a variable which contains data from **prior time steps**.\n","2. Lag features are a classical way for transforming a time series forecasting problem into a **supervised learning problem**.\n","3. Let's say you are predicting the stock price for a company. So, the previous day’s stock price is vital in making predictions right? So, this means that the value at time t is greatly affected by the value at time t-1. The past values are known as lags, so **t-1 is lag 1, t-2 is lag 2,** and so on."]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:39:51.642301Z","iopub.status.busy":"2022-07-10T12:39:51.641912Z","iopub.status.idle":"2022-07-10T12:40:03.075857Z","shell.execute_reply":"2022-07-10T12:40:03.074912Z","shell.execute_reply.started":"2022-07-10T12:39:51.642238Z"},"trusted":true},"outputs":[],"source":["#create the lag lists for lag1, lag2, lag3 with prior time data\n","lag_list = [1, 2, 3]\n","#name the lag list features\n","for lag in lag_list:\n","    ft_name = ('item_cnt_shifted%s' % lag)\n","    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n","    # Fill the empty shifted features with 0\n","    train_monthly[ft_name].fillna(0, inplace=True)"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:03.077659Z","iopub.status.busy":"2022-07-10T12:40:03.076952Z","iopub.status.idle":"2022-07-10T12:40:03.431137Z","shell.execute_reply":"2022-07-10T12:40:03.430116Z","shell.execute_reply.started":"2022-07-10T12:40:03.077626Z"},"trusted":true},"outputs":[],"source":["#generate monthly_trend variable\n","train_monthly['item_trend'] = train_monthly['item_cnt']\n","\n","for lag in lag_list:\n","    ft_name = ('item_cnt_shifted%s' % lag)\n","    train_monthly['item_trend'] -= train_monthly[ft_name]\n","\n","train_monthly['item_trend'] /= len(lag_list) + 1\n","train_monthly.head().T"]},{"cell_type":"markdown","metadata":{},"source":["We need to pre-define the train and test datasets for this time series data.\n","\n","The test set is in the future, so we simulate the **same distribution** on our train/validation split.\n","\n","Our **train set** will be the first 3-27 blocks, **validation** will be last 5 blocks (28-32) and **test** will be block 33.\n","\n","(We can leave out the first 3 months because we use a 3 month window to generate features, so these first 3 month won't have really windowed useful features.)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:03.433033Z","iopub.status.busy":"2022-07-10T12:40:03.432581Z","iopub.status.idle":"2022-07-10T12:40:09.385082Z","shell.execute_reply":"2022-07-10T12:40:09.384301Z","shell.execute_reply.started":"2022-07-10T12:40:03.432988Z"},"trusted":true},"outputs":[],"source":["#query the train,test, and validation sets with pre-defined blocks\n","train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\n","validation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\n","test_set = train_monthly.query('date_block_num == 33').copy()\n","#deal with missing data\n","train_set.dropna(subset=['item_cnt_month'], inplace=True)\n","validation_set.dropna(subset=['item_cnt_month'], inplace=True)\n","\n","train_set.dropna(inplace=True)\n","validation_set.dropna(inplace=True)\n","\n","print('Train set records:', train_set.shape[0])\n","print('Validation set records:', validation_set.shape[0])\n","print('Test set records:', test_set.shape[0])\n"]},{"cell_type":"markdown","metadata":{},"source":["**Mean encoding** can be quite helpful for the model which we use (a gradient boosting model), and represent the features in a better way. Since we have a lot of different values for shop_ids, item_ids, this can also help in reducing cardinality."]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:09.386947Z","iopub.status.busy":"2022-07-10T12:40:09.386056Z","iopub.status.idle":"2022-07-10T12:40:10.593338Z","shell.execute_reply":"2022-07-10T12:40:10.592228Z","shell.execute_reply.started":"2022-07-10T12:40:09.386910Z"},"trusted":true},"outputs":[],"source":[" # Shop mean encoding.\n","gp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\n","gp_shop_mean.columns = ['shop_mean']\n","gp_shop_mean.reset_index(inplace=True)\n","# Item mean encoding.\n","gp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\n","gp_item_mean.columns = ['item_mean']\n","gp_item_mean.reset_index(inplace=True)\n","# Shop with item mean encoding.\n","gp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\n","gp_shop_item_mean.columns = ['shop_item_mean']\n","gp_shop_item_mean.reset_index(inplace=True)\n","# Year mean encoding.\n","gp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\n","gp_year_mean.columns = ['year_mean']\n","gp_year_mean.reset_index(inplace=True)\n","# Month mean encoding.\n","gp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\n","gp_month_mean.columns = ['month_mean']\n","gp_month_mean.reset_index(inplace=True)"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:10.596045Z","iopub.status.busy":"2022-07-10T12:40:10.594857Z","iopub.status.idle":"2022-07-10T12:40:20.619450Z","shell.execute_reply":"2022-07-10T12:40:20.618125Z","shell.execute_reply.started":"2022-07-10T12:40:10.595992Z"},"trusted":true},"outputs":[],"source":["# Add mean encoding features to train set.\n","train_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\n","train_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\n","train_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\n","train_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\n","train_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n","# Add mean encoding features to validation set.\n","validation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\n","validation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\n","validation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\n","validation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\n","validation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:20.621050Z","iopub.status.busy":"2022-07-10T12:40:20.620727Z","iopub.status.idle":"2022-07-10T12:40:23.032770Z","shell.execute_reply":"2022-07-10T12:40:23.031840Z","shell.execute_reply.started":"2022-07-10T12:40:20.621022Z"},"trusted":true},"outputs":[],"source":["# Create train and validation sets and labels. \n","X_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\n","Y_train = train_set['item_cnt_month'].astype(int)\n","X_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\n","Y_validation = validation_set['item_cnt_month'].astype(int)"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:23.034211Z","iopub.status.busy":"2022-07-10T12:40:23.033891Z","iopub.status.idle":"2022-07-10T12:40:23.388467Z","shell.execute_reply":"2022-07-10T12:40:23.387348Z","shell.execute_reply.started":"2022-07-10T12:40:23.034182Z"},"trusted":true},"outputs":[],"source":["int_features = ['shop_id', 'item_id', 'year', 'month']\n","\n","X_train[int_features] = X_train[int_features].astype('int32')\n","X_validation[int_features] = X_validation[int_features].astype('int32')"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:23.390014Z","iopub.status.busy":"2022-07-10T12:40:23.389710Z","iopub.status.idle":"2022-07-10T12:40:28.201224Z","shell.execute_reply":"2022-07-10T12:40:28.200166Z","shell.execute_reply.started":"2022-07-10T12:40:23.389987Z"},"trusted":true},"outputs":[],"source":["latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\n","X_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\n","X_test['year'] = 2015\n","X_test['month'] = 9\n","X_test.drop('item_cnt_month', axis=1, inplace=True)\n","\n","X_test = X_test[X_train.columns]"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:40:28.203418Z","iopub.status.busy":"2022-07-10T12:40:28.202940Z","iopub.status.idle":"2022-07-10T12:42:07.992513Z","shell.execute_reply":"2022-07-10T12:42:07.991378Z","shell.execute_reply.started":"2022-07-10T12:40:28.203372Z"},"trusted":true},"outputs":[],"source":["sets = [X_train, X_validation, X_test]\n","# Replace missing values with the median of each shop.            \n","for dataset in sets:\n","    for shop_id in dataset['shop_id'].unique():\n","        for column in dataset.columns:\n","            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n","            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n","            \n","# Fill remaining missing values on test set with mean.\n","X_test.fillna(X_test.mean(), inplace=True)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:42:07.994166Z","iopub.status.busy":"2022-07-10T12:42:07.993832Z","iopub.status.idle":"2022-07-10T12:42:08.630650Z","shell.execute_reply":"2022-07-10T12:42:08.629591Z","shell.execute_reply.started":"2022-07-10T12:42:07.994135Z"},"trusted":true},"outputs":[],"source":["# I'm dropping \"item_category_id\", we don't have it on test set and would be a little hard to create categories for items that exist only on test set.\n","X_train.drop(['item_category_id'], axis=1, inplace=True)\n","X_validation.drop(['item_category_id'], axis=1, inplace=True)\n","X_test.drop(['item_category_id'], axis=1, inplace=True)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:42:08.632609Z","iopub.status.busy":"2022-07-10T12:42:08.632105Z","iopub.status.idle":"2022-07-10T12:42:08.653015Z","shell.execute_reply":"2022-07-10T12:42:08.652292Z","shell.execute_reply.started":"2022-07-10T12:42:08.632563Z"},"trusted":true},"outputs":[],"source":["X_test.head().T"]},{"cell_type":"markdown","metadata":{},"source":["**4.Modelling**"]},{"cell_type":"markdown","metadata":{},"source":["**4.1XGBoost**\n","\n","A gradient boosting model works by adding predictors to an ensemble in a sequential fashion, with the new predictor being fit to the residual errors made by the previous predictor."]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:42:08.654848Z","iopub.status.busy":"2022-07-10T12:42:08.654430Z","iopub.status.idle":"2022-07-10T12:42:08.915845Z","shell.execute_reply":"2022-07-10T12:42:08.914687Z","shell.execute_reply.started":"2022-07-10T12:42:08.654813Z"},"trusted":true},"outputs":[],"source":["# Use part of features on XGBoost.\n","xgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', 'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', 'shop_item_mean', 'item_trend', 'mean_item_cnt']\n","xgb_train = X_train[xgb_features]\n","xgb_val = X_validation[xgb_features]\n","xgb_test = X_test[xgb_features]"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:42:08.917617Z","iopub.status.busy":"2022-07-10T12:42:08.917209Z","iopub.status.idle":"2022-07-10T12:49:49.254004Z","shell.execute_reply":"2022-07-10T12:49:49.252989Z","shell.execute_reply.started":"2022-07-10T12:42:08.917575Z"},"trusted":true},"outputs":[],"source":["#set up and fit the model\n","from xgboost import XGBRegressor\n","xgb_model = XGBRegressor(max_depth=10,  #maximum depth of the decision trees being trained,if this value increase, more complex is the model,more likely to be overfitted.default=6\n","                         n_estimators=500,  #500 trees are ensembled for efficient learning of data \n","                         min_child_weight=1000,  #default=1.建立每节需要的样本数量\n","                         colsample_bytree=0.7, # The subsample ratio of rows as each tree is generated.Subsampling is done once per iteration.\n","                         subsample=0.7,  #default=1.the proportion regarded as training data. In this case, we use 70% as training data.\n","                         eta=0.3, \n","                         seed=0)\n","xgb_model.fit(xgb_train, \n","              Y_train, \n","              eval_metric=\"rmse\", \n","              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n","              verbose=20, \n","              early_stopping_rounds=20)"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:49:49.255665Z","iopub.status.busy":"2022-07-10T12:49:49.255327Z","iopub.status.idle":"2022-07-10T12:49:49.485516Z","shell.execute_reply":"2022-07-10T12:49:49.482934Z","shell.execute_reply.started":"2022-07-10T12:49:49.255635Z"},"trusted":true},"outputs":[],"source":["#get feature importance and visualizing\n","from xgboost import plot_importance\n","plt.rcParams[\"figure.figsize\"] = (15, 10)\n","plot_importance(xgb_model)\n","plt.show()"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:49:49.488188Z","iopub.status.busy":"2022-07-10T12:49:49.487356Z","iopub.status.idle":"2022-07-10T12:50:35.134969Z","shell.execute_reply":"2022-07-10T12:50:35.134161Z","shell.execute_reply.started":"2022-07-10T12:49:49.488135Z"},"trusted":true},"outputs":[],"source":["#predict the model\n","xgb_train_pred = xgb_model.predict(xgb_train)\n","xgb_val_pred = xgb_model.predict(xgb_val)\n","xgb_test_pred = xgb_model.predict(xgb_test)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:50:35.137055Z","iopub.status.busy":"2022-07-10T12:50:35.136430Z","iopub.status.idle":"2022-07-10T12:50:35.191705Z","shell.execute_reply":"2022-07-10T12:50:35.190990Z","shell.execute_reply.started":"2022-07-10T12:50:35.137010Z"},"trusted":true},"outputs":[],"source":["#get the error:rmse\n","print('Train rmse:', np.sqrt(mean_squared_error(Y_train, xgb_train_pred)))\n","print('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)))"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:50:35.193476Z","iopub.status.busy":"2022-07-10T12:50:35.192743Z","iopub.status.idle":"2022-07-10T12:50:35.200565Z","shell.execute_reply":"2022-07-10T12:50:35.199529Z","shell.execute_reply.started":"2022-07-10T12:50:35.193443Z"},"trusted":true},"outputs":[],"source":["xgb_test_pred"]},{"cell_type":"markdown","metadata":{},"source":["**4.2 Random Forest Modelling**"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:50:35.202687Z","iopub.status.busy":"2022-07-10T12:50:35.202043Z","iopub.status.idle":"2022-07-10T12:50:35.541847Z","shell.execute_reply":"2022-07-10T12:50:35.540886Z","shell.execute_reply.started":"2022-07-10T12:50:35.202643Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","#select the useful feature in this model\n","rf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', 'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\n","rf_train = X_train[rf_features]\n","rf_val = X_validation[rf_features]\n","rf_test = X_test[rf_features]"]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:50:35.543731Z","iopub.status.busy":"2022-07-10T12:50:35.543414Z","iopub.status.idle":"2022-07-10T12:54:34.719815Z","shell.execute_reply":"2022-07-10T12:54:34.718447Z","shell.execute_reply.started":"2022-07-10T12:50:35.543702Z"},"trusted":true},"outputs":[],"source":["#build and fit model\n","rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\n","rf_model.fit(rf_train, Y_train)"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:54:34.722439Z","iopub.status.busy":"2022-07-10T12:54:34.721709Z","iopub.status.idle":"2022-07-10T12:54:40.440762Z","shell.execute_reply":"2022-07-10T12:54:40.439672Z","shell.execute_reply.started":"2022-07-10T12:54:34.722391Z"},"trusted":true},"outputs":[],"source":["#make predictions\n","rf_train_pred = rf_model.predict(rf_train)\n","rf_val_pred = rf_model.predict(rf_val)\n","rf_test_pred = rf_model.predict(rf_test)"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:54:40.442779Z","iopub.status.busy":"2022-07-10T12:54:40.442267Z","iopub.status.idle":"2022-07-10T12:54:40.499718Z","shell.execute_reply":"2022-07-10T12:54:40.498455Z","shell.execute_reply.started":"2022-07-10T12:54:40.442716Z"},"trusted":true},"outputs":[],"source":["#show the result and error\n","print('Train rmse:', np.sqrt(mean_squared_error(Y_train, rf_train_pred)))\n","print('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, rf_val_pred)))"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:54:40.501424Z","iopub.status.busy":"2022-07-10T12:54:40.501052Z","iopub.status.idle":"2022-07-10T12:54:42.531160Z","shell.execute_reply":"2022-07-10T12:54:42.530096Z","shell.execute_reply.started":"2022-07-10T12:54:40.501390Z"},"trusted":true},"outputs":[],"source":["#get the correlation between the predictions using different models\n","plt.scatter(rf_val_pred, xgb_val_pred)"]},{"cell_type":"markdown","metadata":{},"source":["**4.3 Linear Regression**"]},{"cell_type":"code","execution_count":78,"metadata":{"execution":{"iopub.execute_input":"2022-07-10T12:58:30.454842Z","iopub.status.busy":"2022-07-10T12:58:30.454399Z","iopub.status.idle":"2022-07-10T12:58:34.052047Z","shell.execute_reply":"2022-07-10T12:58:34.050467Z","shell.execute_reply.started":"2022-07-10T12:58:30.454806Z"},"trusted":true},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","lr=LinearRegression()\n","#fit the model,replacing the time\n","X = train_set.drop('item_cnt_month', axis=1)\n","#dealing with missing data filling with 0\n","Lr_X = X.dropna(axis=0)\n","y = train_set['item_cnt_month'].dropna(axis=1)\n","#fit the model\n","lr.fit(Lr_X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.731768Z","iopub.status.idle":"2022-07-10T12:54:46.732180Z","shell.execute_reply":"2022-07-10T12:54:46.732012Z","shell.execute_reply.started":"2022-07-10T12:54:46.731992Z"},"trusted":true},"outputs":[],"source":["#make the prediction with linear regression model\n","lr_prediction=lr.predict(X)"]},{"cell_type":"markdown","metadata":{},"source":["**4.4 LSTM Modelling**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.733803Z","iopub.status.idle":"2022-07-10T12:54:46.734666Z","shell.execute_reply":"2022-07-10T12:54:46.734469Z","shell.execute_reply.started":"2022-07-10T12:54:46.734446Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.models import Sequential\n","from keras.layers import LSTM, Dense, Activation\n","from keras import backend as K\n","from nested_lstm import NestedLSTMCell, NestedLSTM\n","#for imply the nested LSTM model, we need to insert a support document from:https://github.com/titu1994/Nested-LSTM/tree/5ad1939c18b9ce66b16fab0f48e0528e0c584876\n","from __future__ import absolute_import\n","import warnings\n","\n","from keras import backend as K\n","from keras import activations\n","from keras import initializers\n","from keras import regularizers\n","from keras import constraints\n","from keras.engine import Layer\n","from keras.engine import InputSpec\n","from keras.legacy import interfaces\n","from keras.layers import RNN\n","from keras.layers.recurrent import _generate_dropout_mask\n","from keras.layers import LSTMCell, LSTM\n","\n","\n","class NestedLSTMCell(Layer):\n","    \"\"\"Nested NestedLSTM Cell class.\n","\n","    Derived from the paper [Nested LSTMs](https://arxiv.org/abs/1801.10308)\n","    Ref: [Tensorflow implementation](https://github.com/hannw/nlstm)\n","\n","    # Arguments\n","        units: Positive integer, dimensionality of the output space.\n","        depth: Depth of nesting of the memory component.\n","        activation: Activation function to use\n","            (see [activations](../activations.md)).\n","            If you pass None, no activation is applied\n","            (ie. \"linear\" activation: `a(x) = x`).\n","        recurrent_activation: Activation function to use\n","            for the recurrent step\n","            (see [activations](../activations.md)).\n","        cell_activation: Activation function of the first cell gate.\n","            Note that in the paper only the first cell_activation is identity.\n","            (see [activations](../activations.md)).\n","        use_bias: Boolean, whether the layer uses a bias vector.\n","        kernel_initializer: Initializer for the `kernel` weights matrix,\n","            used for the linear transformation of the inputs\n","            (see [initializers](../initializers.md)).\n","        recurrent_initializer: Initializer for the `recurrent_kernel`\n","            weights matrix,\n","            used for the linear transformation of the recurrent state\n","            (see [initializers](../initializers.md)).\n","        bias_initializer: Initializer for the bias vector\n","            (see [initializers](../initializers.md)).\n","        unit_forget_bias: Boolean.\n","            If True, add 1 to the bias of the forget gate at initialization.\n","            Setting it to true will also force `bias_initializer=\"zeros\"`.\n","            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n","        kernel_regularizer: Regularizer function applied to\n","            the `kernel` weights matrix\n","            (see [regularizer](../regularizers.md)).\n","        recurrent_regularizer: Regularizer function applied to\n","            the `recurrent_kernel` weights matrix\n","            (see [regularizer](../regularizers.md)).\n","        bias_regularizer: Regularizer function applied to the bias vector\n","            (see [regularizer](../regularizers.md)).\n","        kernel_constraint: Constraint function applied to\n","            the `kernel` weights matrix\n","            (see [constraints](../constraints.md)).\n","        recurrent_constraint: Constraint function applied to\n","            the `recurrent_kernel` weights matrix\n","            (see [constraints](../constraints.md)).\n","        bias_constraint: Constraint function applied to the bias vector\n","            (see [constraints](../constraints.md)).\n","        dropout: Float between 0 and 1.\n","            Fraction of the units to drop for\n","            the linear transformation of the inputs.\n","        recurrent_dropout: Float between 0 and 1.\n","            Fraction of the units to drop for\n","            the linear transformation of the recurrent state.\n","        implementation: Implementation mode, must be 2.\n","            Mode 1 will structure its operations as a larger number of\n","            smaller dot products and additions, whereas mode 2 will\n","            batch them into fewer, larger operations. These modes will\n","            have different performance profiles on different hardware and\n","            for different applications.\n","    \"\"\"\n","\n","    def __init__(self, units, depth,\n","                 activation='tanh',\n","                 recurrent_activation='sigmoid',\n","                 cell_activation='linear',\n","                 use_bias=True,\n","                 kernel_initializer='glorot_uniform',\n","                 recurrent_initializer='orthogonal',\n","                 bias_initializer='zeros',\n","                 unit_forget_bias=False,\n","                 kernel_regularizer=None,\n","                 recurrent_regularizer=None,\n","                 bias_regularizer=None,\n","                 kernel_constraint=None,\n","                 recurrent_constraint=None,\n","                 bias_constraint=None,\n","                 dropout=0.,\n","                 recurrent_dropout=0.,\n","                 implementation=2,\n","                 **kwargs):\n","        super(NestedLSTMCell, self).__init__(**kwargs)\n","\n","        if depth < 1:\n","            raise ValueError(\"`depth` must be at least 1. For better performance, consider using depth > 1.\")\n","\n","        if implementation != 1:\n","            warnings.warn(\n","                \"Nested LSTMs only supports implementation 2 for the moment. Defaulting to implementation = 2\")\n","            implementation = 2\n","\n","        self.units = units\n","        self.depth = depth\n","        self.activation = activations.get(activation)\n","        self.recurrent_activation = activations.get(recurrent_activation)\n","        self.cell_activation = activations.get(cell_activation)\n","        self.use_bias = use_bias\n","\n","        self.kernel_initializer = initializers.get(kernel_initializer)\n","        self.recurrent_initializer = initializers.get(recurrent_initializer)\n","        self.bias_initializer = initializers.get(bias_initializer)\n","        self.unit_forget_bias = unit_forget_bias\n","\n","        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n","        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n","        self.bias_regularizer = regularizers.get(bias_regularizer)\n","\n","        self.kernel_constraint = constraints.get(kernel_constraint)\n","        self.recurrent_constraint = constraints.get(recurrent_constraint)\n","        self.bias_constraint = constraints.get(bias_constraint)\n","\n","        self.dropout = min(1., max(0., dropout))\n","        self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n","        self.implementation = implementation\n","        self.state_size = tuple([self.units] * (self.depth + 1))\n","        self._dropout_mask = None\n","        self._nested_recurrent_masks = None\n","\n","    def build(self, input_shape):\n","        input_dim = input_shape[-1]\n","        self.kernels = []\n","        self.biases = []\n","\n","        for i in range(self.depth):\n","            if i == 0:\n","                input_kernel = self.add_weight(shape=(input_dim, self.units * 4),\n","                                               name='input_kernel_%d' % (i + 1),\n","                                               initializer=self.kernel_initializer,\n","                                               regularizer=self.kernel_regularizer,\n","                                               constraint=self.kernel_constraint)\n","                hidden_kernel = self.add_weight(shape=(self.units, self.units * 4),\n","                                                name='kernel_%d' % (i + 1),\n","                                                initializer=self.recurrent_initializer,\n","                                                regularizer=self.recurrent_regularizer,\n","                                                constraint=self.recurrent_constraint)\n","                kernel = K.concatenate([input_kernel, hidden_kernel], axis=0)\n","            else:\n","                kernel = self.add_weight(shape=(self.units * 2, self.units * 4),\n","                                         name='kernel_%d' % (i + 1),\n","                                         initializer=self.recurrent_initializer,\n","                                         regularizer=self.recurrent_regularizer,\n","                                         constraint=self.recurrent_constraint)\n","            self.kernels.append(kernel)\n","\n","        if self.use_bias:\n","            if self.unit_forget_bias:\n","                def bias_initializer(_, *args, **kwargs):\n","                    return K.concatenate([\n","                        self.bias_initializer((self.units,), *args, **kwargs),\n","                        initializers.Ones()((self.units,), *args, **kwargs),\n","                        self.bias_initializer((self.units * 2,), *args, **kwargs),\n","                    ])\n","            else:\n","                bias_initializer = self.bias_initializer\n","\n","            for i in range(self.depth):\n","                bias = self.add_weight(shape=(self.units * 4,),\n","                                       name='bias_%d' % (i + 1),\n","                                       initializer=bias_initializer,\n","                                       regularizer=self.bias_regularizer,\n","                                       constraint=self.bias_constraint)\n","                self.biases.append(bias)\n","        else:\n","            self.biases = None\n","\n","        self.built = True\n","\n","    def call(self, inputs, states, training=None):\n","        if 0 < self.dropout < 1 and self._dropout_mask is None:\n","            self._dropout_mask = _generate_dropout_mask(\n","                K.ones_like(inputs),\n","                self.dropout,\n","                training=training,\n","                count=1)\n","        if (0 < self.recurrent_dropout < 1 and\n","                self._nested_recurrent_masks is None):\n","            _nested_recurrent_mask = _generate_dropout_mask(\n","                K.ones_like(states[0]),\n","                self.recurrent_dropout,\n","                training=training,\n","                count=self.depth)\n","            self._nested_recurrent_masks = _nested_recurrent_mask\n","\n","        # dropout matrices for input units\n","        dp_mask = self._dropout_mask\n","        # dropout matrices for recurrent units\n","        rec_dp_masks = self._nested_recurrent_masks\n","\n","        h_tm1 = states[0]  # previous memory state\n","        c_tm1 = states[1:self.depth + 1]  # previous carry states\n","\n","        if 0. < self.dropout < 1.:\n","            inputs *= dp_mask[0]\n","\n","        h, c = self.nested_recurrence(inputs,\n","                                      hidden_state=h_tm1,\n","                                      cell_states=c_tm1,\n","                                      recurrent_masks=rec_dp_masks,\n","                                      current_depth=0)\n","\n","        if 0 < self.dropout + self.recurrent_dropout:\n","            if training is None:\n","                h._uses_learning_phase = True\n","        return h, c\n","\n","    def nested_recurrence(self, inputs, hidden_state, cell_states, recurrent_masks, current_depth):\n","        h_state = hidden_state\n","        c_state = cell_states[current_depth]\n","\n","        if 0.0 < self.recurrent_dropout <= 1. and recurrent_masks is not None:\n","            hidden_state = h_state * recurrent_masks[current_depth]\n","\n","        ip = K.concatenate([inputs, hidden_state], axis=-1)\n","        gate_inputs = K.dot(ip, self.kernels[current_depth])\n","\n","        if self.use_bias:\n","            gate_inputs = K.bias_add(gate_inputs, self.biases[current_depth])\n","\n","        i = gate_inputs[:, :self.units]  # input gate\n","        f = gate_inputs[:, self.units * 2: self.units * 3]  # forget gate\n","        c = gate_inputs[:, self.units: 2 * self.units]  # new input\n","        o = gate_inputs[:, self.units * 3: self.units * 4]  # output gate\n","\n","        inner_hidden = c_state * self.recurrent_activation(f)\n","\n","        if current_depth == 0:\n","            inner_input = self.recurrent_activation(i) + self.cell_activation(c)\n","        else:\n","            inner_input = self.recurrent_activation(i) + self.activation(c)\n","\n","        if (current_depth == self.depth - 1):\n","            new_c = inner_hidden + inner_input\n","            new_cs = [new_c]\n","        else:\n","            new_c, new_cs = self.nested_recurrence(inner_input,\n","                                                   hidden_state=inner_hidden,\n","                                                   cell_states=cell_states,\n","                                                   recurrent_masks=recurrent_masks,\n","                                                   current_depth=current_depth + 1)\n","\n","        new_h = self.activation(new_c) * self.recurrent_activation(o)\n","        new_cs = [new_h] + new_cs\n","\n","        return new_h, new_cs\n","\n","    def get_config(self):\n","        config = {'units': self.units,\n","                  'depth': self.depth,\n","                  'activation': activations.serialize(self.activation),\n","                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n","                  'cell_activation': activations.serialize(self.cell_activation),\n","                  'use_bias': self.use_bias,\n","                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n","                  'bias_initializer': initializers.serialize(self.bias_initializer),\n","                  'unit_forget_bias': self.unit_forget_bias,\n","                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n","                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n","                  'bias_constraint': constraints.serialize(self.bias_constraint),\n","                  'dropout': self.dropout,\n","                  'recurrent_dropout': self.recurrent_dropout,\n","                  'implementation': self.implementation}\n","        base_config = super(NestedLSTMCell, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","\n","class NestedLSTM(RNN):\n","    \"\"\"Nested Long-Short-Term-Memory layer - [Nested LSTMs](https://arxiv.org/abs/1801.10308).\n","\n","    # Arguments\n","        units: Positive integer, dimensionality of the output space.\n","        depth: Depth of nesting of the memory component.\n","        activation: Activation function to use\n","            (see [activations](../activations.md)).\n","            If you pass None, no activation is applied\n","            (ie. \"linear\" activation: `a(x) = x`).\n","        recurrent_activation: Activation function to use\n","            for the recurrent step\n","            (see [activations](../activations.md)).\n","        cell_activation: Activation function of the first cell gate.\n","            Note that in the paper only the first cell_activation is identity.\n","            (see [activations](../activations.md)).\n","        use_bias: Boolean, whether the layer uses a bias vector.\n","        kernel_initializer: Initializer for the `kernel` weights matrix,\n","            used for the linear transformation of the inputs.\n","            (see [initializers](../initializers.md)).\n","        recurrent_initializer: Initializer for the `recurrent_kernel`\n","            weights matrix,\n","            used for the linear transformation of the recurrent state.\n","            (see [initializers](../initializers.md)).\n","        bias_initializer: Initializer for the bias vector\n","            (see [initializers](../initializers.md)).\n","        unit_forget_bias: Boolean.\n","            If True, add 1 to the bias of the forget gate at initialization.\n","            Setting it to true will also force `bias_initializer=\"zeros\"`.\n","            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n","        kernel_regularizer: Regularizer function applied to\n","            the `kernel` weights matrix\n","            (see [regularizer](../regularizers.md)).\n","        recurrent_regularizer: Regularizer function applied to\n","            the `recurrent_kernel` weights matrix\n","            (see [regularizer](../regularizers.md)).\n","        bias_regularizer: Regularizer function applied to the bias vector\n","            (see [regularizer](../regularizers.md)).\n","        activity_regularizer: Regularizer function applied to\n","            the output of the layer (its \"activation\").\n","            (see [regularizer](../regularizers.md)).\n","        kernel_constraint: Constraint function applied to\n","            the `kernel` weights matrix\n","            (see [constraints](../constraints.md)).\n","        recurrent_constraint: Constraint function applied to\n","            the `recurrent_kernel` weights matrix\n","            (see [constraints](../constraints.md)).\n","        bias_constraint: Constraint function applied to the bias vector\n","            (see [constraints](../constraints.md)).\n","        dropout: Float between 0 and 1.\n","            Fraction of the units to drop for\n","            the linear transformation of the inputs.\n","        recurrent_dropout: Float between 0 and 1.\n","            Fraction of the units to drop for\n","            the linear transformation of the recurrent state.\n","        implementation: Implementation mode, either 1 or 2.\n","            Mode 1 will structure its operations as a larger number of\n","            smaller dot products and additions, whereas mode 2 will\n","            batch them into fewer, larger operations. These modes will\n","            have different performance profiles on different hardware and\n","            for different applications.\n","        return_sequences: Boolean. Whether to return the last output.\n","            in the output sequence, or the full sequence.\n","        return_state: Boolean. Whether to return the last state\n","            in addition to the output.\n","        go_backwards: Boolean (default False).\n","            If True, process the input sequence backwards and return the\n","            reversed sequence.\n","        stateful: Boolean (default False). If True, the last state\n","            for each sample at index i in a batch will be used as initial\n","            state for the sample of index i in the following batch.\n","        unroll: Boolean (default False).\n","            If True, the network will be unrolled,\n","            else a symbolic loop will be used.\n","            Unrolling can speed-up a RNN,\n","            although it tends to be more memory-intensive.\n","            Unrolling is only suitable for short sequences.\n","\n","    # References\n","        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n","        - [Learning to forget: Continual prediction with NestedLSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n","        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n","        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n","        - [Nested LSTMs](https://arxiv.org/abs/1801.10308)\n","    \"\"\"\n","\n","    @interfaces.legacy_recurrent_support\n","    def __init__(self, units, depth,\n","                 activation='tanh',\n","                 recurrent_activation='sigmoid',\n","                 cell_activation='linear',\n","                 use_bias=True,\n","                 kernel_initializer='glorot_uniform',\n","                 recurrent_initializer='orthogonal',\n","                 bias_initializer='zeros',\n","                 unit_forget_bias=False,\n","                 kernel_regularizer=None,\n","                 recurrent_regularizer=None,\n","                 bias_regularizer=None,\n","                 activity_regularizer=None,\n","                 kernel_constraint=None,\n","                 recurrent_constraint=None,\n","                 bias_constraint=None,\n","                 dropout=0.,\n","                 recurrent_dropout=0.,\n","                 implementation=1,\n","                 return_sequences=False,\n","                 return_state=False,\n","                 go_backwards=False,\n","                 stateful=False,\n","                 unroll=False,\n","                 **kwargs):\n","        if implementation == 0:\n","            warnings.warn('`implementation=0` has been deprecated, '\n","                          'and now defaults to `implementation=2`.'\n","                          'Please update your layer call.')\n","        if K.backend() == 'theano':\n","            warnings.warn(\n","                'RNN dropout is no longer supported with the Theano backend '\n","                'due to technical limitations. '\n","                'You can either set `dropout` and `recurrent_dropout` to 0, '\n","                'or use the TensorFlow backend.')\n","            dropout = 0.\n","            recurrent_dropout = 0.\n","\n","        cell = NestedLSTMCell(units, depth,\n","                              activation=activation,\n","                              recurrent_activation=recurrent_activation,\n","                              cell_activation=cell_activation,\n","                              use_bias=use_bias,\n","                              kernel_initializer=kernel_initializer,\n","                              recurrent_initializer=recurrent_initializer,\n","                              unit_forget_bias=unit_forget_bias,\n","                              bias_initializer=bias_initializer,\n","                              kernel_regularizer=kernel_regularizer,\n","                              recurrent_regularizer=recurrent_regularizer,\n","                              bias_regularizer=bias_regularizer,\n","                              kernel_constraint=kernel_constraint,\n","                              recurrent_constraint=recurrent_constraint,\n","                              bias_constraint=bias_constraint,\n","                              dropout=dropout,\n","                              recurrent_dropout=recurrent_dropout,\n","                              implementation=implementation)\n","        super(NestedLSTM, self).__init__(cell,\n","                                         return_sequences=return_sequences,\n","                                         return_state=return_state,\n","                                         go_backwards=go_backwards,\n","                                         stateful=stateful,\n","                                         unroll=unroll,\n","                                         **kwargs)\n","        self.activity_regularizer = regularizers.get(activity_regularizer)\n","\n","    def call(self, inputs, mask=None, training=None, initial_state=None, constants=None):\n","        self.cell._dropout_mask = None\n","        self.cell._nested_recurrent_masks = None\n","        return super(NestedLSTM, self).call(inputs,\n","                                            mask=mask,\n","                                            training=training,\n","                                            initial_state=initial_state,\n","                                            constants=constants)\n","\n","    @property\n","    def units(self):\n","        return self.cell.units\n","\n","    @property\n","    def depth(self):\n","        return self.cell.depth\n","\n","    @property\n","    def activation(self):\n","        return self.cell.activation\n","\n","    @property\n","    def recurrent_activation(self):\n","        return self.cell.recurrent_activation\n","\n","    @property\n","    def cell_activation(self):\n","        return self.cell.cell_activation\n","\n","    @property\n","    def use_bias(self):\n","        return self.cell.use_bias\n","\n","    @property\n","    def kernel_initializer(self):\n","        return self.cell.kernel_initializer\n","\n","    @property\n","    def recurrent_initializer(self):\n","        return self.cell.recurrent_initializer\n","\n","    @property\n","    def bias_initializer(self):\n","        return self.cell.bias_initializer\n","\n","    @property\n","    def unit_forget_bias(self):\n","        return self.cell.unit_forget_bias\n","\n","    @property\n","    def kernel_regularizer(self):\n","        return self.cell.kernel_regularizer\n","\n","    @property\n","    def recurrent_regularizer(self):\n","        return self.cell.recurrent_regularizer\n","\n","    @property\n","    def bias_regularizer(self):\n","        return self.cell.bias_regularizer\n","\n","    @property\n","    def kernel_constraint(self):\n","        return self.cell.kernel_constraint\n","\n","    @property\n","    def recurrent_constraint(self):\n","        return self.cell.recurrent_constraint\n","\n","    @property\n","    def bias_constraint(self):\n","        return self.cell.bias_constraint\n","\n","    @property\n","    def dropout(self):\n","        return self.cell.dropout\n","\n","    @property\n","    def recurrent_dropout(self):\n","        return self.cell.recurrent_dropout\n","\n","    @property\n","    def implementation(self):\n","        return self.cell.implementation\n","\n","    def get_config(self):\n","        config = {'units': self.units,\n","                  'depth': self.depth,\n","                  'activation': activations.serialize(self.activation),\n","                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n","                  'cell_activation': activations.serialize(self.cell_activation),\n","                  'use_bias': self.use_bias,\n","                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n","                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n","                  'bias_initializer': initializers.serialize(self.bias_initializer),\n","                  'unit_forget_bias': self.unit_forget_bias,\n","                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n","                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n","                  'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n","                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n","                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n","                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n","                  'bias_constraint': constraints.serialize(self.bias_constraint),\n","                  'dropout': self.dropout,\n","                  'recurrent_dropout': self.recurrent_dropout,\n","                  'implementation': self.implementation}\n","        base_config = super(NestedLSTM, self).get_config()\n","        del base_config['cell']\n","        return dict(list(base_config.items()) + list(config.items()))\n","\n","    @classmethod\n","    def from_config(cls, config):\n","        if 'implementation' in config and config['implementation'] == 0:\n","            config['implementation'] = 2\n","        return cls(**config)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.735877Z","iopub.status.idle":"2022-07-10T12:54:46.736293Z","shell.execute_reply":"2022-07-10T12:54:46.736108Z","shell.execute_reply.started":"2022-07-10T12:54:46.736090Z"},"trusted":true},"outputs":[],"source":["from IPython.display import Image\n","Image(\"../input/nested-lstm/2022-07-07 8.59.16.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.737450Z","iopub.status.idle":"2022-07-10T12:54:46.737795Z","shell.execute_reply":"2022-07-10T12:54:46.737647Z","shell.execute_reply.started":"2022-07-10T12:54:46.737631Z"},"trusted":true},"outputs":[],"source":["from IPython.display import Image\n","Image(\"../input/nested-lstm/2022-07-07 8.59.06.png\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.739564Z","iopub.status.idle":"2022-07-10T12:54:46.740128Z","shell.execute_reply":"2022-07-10T12:54:46.739974Z","shell.execute_reply.started":"2022-07-10T12:54:46.739956Z"},"trusted":true},"outputs":[],"source":["#establish the model\n","LSTM = Sequential()\n","LSTM.add(NestedLSTM(40, input_shape=(33, 1), depth=2, dropout=0.0, recurrent_dropout=0.0))\n","LSTM.add(Dense(1))\n","# use adam optimizer \n","LSTM.compile(loss='mse',optimizer='adam',metrics=['mean_squared_error'])\n","LSTM.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.740928Z","iopub.status.idle":"2022-07-10T12:54:46.741770Z","shell.execute_reply":"2022-07-10T12:54:46.741549Z","shell.execute_reply.started":"2022-07-10T12:54:46.741513Z"},"trusted":true},"outputs":[],"source":["#generate a dataframe df,deal with datatime with year-month format\n","LSTM_df = sales.groupby([sales.date.apply(lambda x: x.strftime('%Y-%m')),'item_id','shop_id']).sum().reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.742657Z","iopub.status.idle":"2022-07-10T12:54:46.743030Z","shell.execute_reply":"2022-07-10T12:54:46.742851Z","shell.execute_reply.started":"2022-07-10T12:54:46.742834Z"},"trusted":true},"outputs":[],"source":["#table the datatime(year-month)\n","LSTM_df = df[['date','item_id','shop_id','item_cnt_day']]\n","LSTM_df = LSTM_df.pivot_table(index=['item_id','shop_id'], columns='date',values='item_cnt_day',fill_value=0).reset_index()\n","LSTM_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.744012Z","iopub.status.idle":"2022-07-10T12:54:46.744395Z","shell.execute_reply":"2022-07-10T12:54:46.744207Z","shell.execute_reply.started":"2022-07-10T12:54:46.744190Z"},"trusted":true},"outputs":[],"source":["#generate the test dataset with merging method\n","LSTM_df_test = pd.merge(test,LSTM_df, on=['item_id','shop_id'], how='left')\n","LSTM_df_test = df_test.fillna(0) #dealing with missing data filling with 0\n","LSTM_df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.745627Z","iopub.status.idle":"2022-07-10T12:54:46.746411Z","shell.execute_reply":"2022-07-10T12:54:46.746145Z","shell.execute_reply.started":"2022-07-10T12:54:46.746122Z"},"trusted":true},"outputs":[],"source":["LSTM_df_test = LSTM_df_test.drop(labels=['ID', 'shop_id', 'item_id'], axis=1)\n","LSTM_df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.747292Z","iopub.status.idle":"2022-07-10T12:54:46.748477Z","shell.execute_reply":"2022-07-10T12:54:46.748147Z","shell.execute_reply.started":"2022-07-10T12:54:46.748123Z"},"trusted":true},"outputs":[],"source":["#generate the train dataset,define the input sequence\n","TARGET = '2015-10' #set up the target data\n","LSTM_y_train = LSTM_df_test[TARGET]\n","LSTM_X_train = LSTM_df_test.drop(labels=[TARGET], axis=1)\n","LSTM_X_train = LSTM_X_train.as_matrix() \n","#reshape the imput according to (samples, timesteps, features)\n","LSTM_X_train = LSTM_X_train.reshape((214200, 33, 1)) \n","\n","LSTM_y_train = LSTM_y_train.as_matrix()\n","LSTM_y_train = LSTM_y_train.reshape(214200, 1) # Timestep dimension will be squeezed in the target, when the target has to be reshaped to match the same pattern\n","\n","LSTM_X_test = LSTM_df_test.drop(labels=['2013-01'],axis=1)\n","LSTM_X_test = LSTM_X_test.as_matrix()\n","LSTM_X_test = LSTM_X_test.reshape((214200, 33, 1))\n","\n","LSTM_df_test = pd.merge(LSTM_df_test, LSTM_df, on=['item_id','shop_id'], how='left') #merge the test and lstm dataframe together \n","LSTM_df_test = LSTM_df_test.fillna(0) #deal with missing value and fill them with zero\n","LSTM_df_test.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-07-10T12:54:46.749816Z","iopub.status.idle":"2022-07-10T12:54:46.750203Z","shell.execute_reply":"2022-07-10T12:54:46.750048Z","shell.execute_reply.started":"2022-07-10T12:54:46.750031Z"},"trusted":true},"outputs":[],"source":["#fit the model,reduce batch size and increase number of epochs, see where it converges.\n","BATCH = 32\n","LSTM.fit(LSTM_X_train, LSTM_y_train,\n","          batch_size=BATCH,\n","          epochs=50)\n","#Prediction\n","prediction=LSTM.predict(LSTM_df_test,verbose=0)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
